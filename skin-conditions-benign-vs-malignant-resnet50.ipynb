{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9480959,"sourceType":"datasetVersion","datasetId":5766945},{"sourceId":9643572,"sourceType":"datasetVersion","datasetId":5889161}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, preprocessing\nfrom sklearn.utils import class_weight\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\n# Define paths to your dataset directories\ntrain_data_dir = '/kaggle/input/skindata/train'\ntest_data_dir = '/kaggle/input/skindata/test'\n\n# Set image size and parameters\nimg_width, img_height = 150, 150\nbatch_size = 32  \nlearning_rate = 0.0001 \n\n# Create ImageDataGenerators without data augmentation, only rescaling\ntrain_datagen = preprocessing.image.ImageDataGenerator(rescale=1.0 / 255.0)\ntest_datagen = preprocessing.image.ImageDataGenerator(rescale=1.0 / 255.0)\n\n# Load the training dataset\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=True,\n    seed=42\n)\n\n# Load the test dataset\ntest_generator = test_datagen.flow_from_directory(\n    test_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=False\n)\n\n# Data for the bar chart\nlabels = ['benign', 'malignant']\ntraining_data = [1400, 1200]\ntest_data = [360, 300]\n\n# Create the bar chart\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n\n# First bar chart for training data\nax1.bar(labels, training_data, color=['blue', 'goldenrod'])\nax1.set_ylabel('Number of Images')\nax1.set_xlabel('Skin Condition')\nax1.set_title('Training Data Class Distribution')\nax1.grid(True, axis='y')\n\n# Second bar chart for test data\nax2.bar(labels, test_data, color=['blue', 'goldenrod'])\nax2.set_ylabel('Number of Images')\nax2.set_xlabel('Skin Condition')\nax2.set_title('Test Data Class Distribution')\nax2.grid(True, axis='y')\n\n# Adjust spacing between subplots\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n# Display the first 8 images from the training dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Get a batch of images and labels from the training generator\ntrain_images, train_labels = next(train_generator)\n\nplt.figure(figsize=(16, 8))  # Set the figure size\nfor i in range(8):\n    plt.subplot(2, 4, i + 1)  # Arrange in 2 rows and 4 columns\n    plt.imshow(train_images[i])\n    label = 'Benign' if train_labels[i] == 0 else 'Malignant'\n    plt.title(label, color='black', fontsize=14)\n    plt.axis('off')  # Hide axes\n\nplt.tight_layout()\nplt.suptitle(\"Training Dataset - First 8 Images\", fontsize=16, color='black', y=1.05)\nplt.show()\n\n# Display the first 8 images from the test dataset\ntest_images, test_labels = next(test_generator)\n\nplt.figure(figsize=(16, 8))  # Set the figure size\nfor i in range(8):\n    plt.subplot(2, 4, i + 1)  # Arrange in 2 rows and 4 columns\n    plt.imshow(test_images[i])\n    label = 'Benign' if test_labels[i] == 0 else 'Malignant'\n    plt.title(label, color='black', fontsize=14)\n    plt.axis('off')  # Hide axes\n\nplt.tight_layout()\nplt.suptitle(\"Test Dataset - First 8 Images\", fontsize=16, color='black', y=1.05)\nplt.show()\n\n# Calculate class weights\nclass_counts = np.bincount(train_generator.classes)\nweight_for_benign_class = class_counts[0] / sum(class_counts)\nweight_for_malignant_class = class_counts[1] / sum(class_counts)\n\nclass_weight = {0: weight_for_benign_class, 1: weight_for_malignant_class}\n\n# Convert generators to TensorFlow datasets for pre-fetching and caching\ntrain_dataset = tf.data.Dataset.from_generator(\n    lambda: train_generator,\n    output_signature=(\n        tf.TensorSpec(shape=(None, img_width, img_height, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None,), dtype=tf.float32)\n    )\n).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()\n\ntest_dataset = tf.data.Dataset.from_generator(\n    lambda: test_generator,\n    output_signature=(\n        tf.TensorSpec(shape=(None, img_width, img_height, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None,), dtype=tf.float32)\n    )\n).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()\n\n# Define the learning rate scheduler callback\n\nlearning_rate_scheduler = ReduceLROnPlateau(\n    monitor='val_loss',    # Metric to monitor\n    factor=0.5,            # Factor by which the learning rate will be reduced\n    patience=3,            # Number of epochs with no improvement to wait before reducing the learning rate\n    min_lr=1e-6            # Minimum learning rate\n)\n\n# Load the ResNet50 model with pre-trained weights\nresnet_base = keras.applications.ResNet50(\n    weights='imagenet',  \n    include_top=False,   \n    input_shape=(img_width, img_height, 3)  \n)\n\n# Freeze the ResNet50 base model layers\nresnet_base.trainable = False\n\n# Build the model\nmodel = models.Sequential([\n    resnet_base,  \n    layers.GlobalAveragePooling2D(),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.L2(0.001)),\n    layers.Dropout(0.4),\n    layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.L2(0.001)),\n    layers.Dropout(0.3),\n    layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Define the EarlyStopping and modelcheckpoint callback\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\nmodel_checkpoint = keras.callbacks.ModelCheckpoint(\n    'best_model.keras', monitor='val_loss', save_best_only=True\n)\n\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    steps_per_epoch=train_generator.samples // batch_size,\n    epochs=25,\n    validation_data=test_dataset,\n    validation_steps=test_generator.samples // batch_size,\n    class_weight=class_weight,\n    callbacks=[early_stopping, model_checkpoint, learning_rate_scheduler]\n)\n\n# Save the model after the initial training phase\nmodel.save('initial_trained_model.keras')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model is saved. Now, Restart and Clear Cell Outputs to free the RAM for fine tuning","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, preprocessing\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\n\n# Define paths to your dataset directories\ntrain_data_dir = '/kaggle/input/skindata/train'\ntest_data_dir = '/kaggle/input/skindata/test'\n\nimg_width, img_height = 150, 150\nbatch_size = 32  # Adjust as necessary\nlearning_rate = 0.0001\n\n# Create ImageDataGenerators without data augmentation, only rescaling\ntrain_datagen = preprocessing.image.ImageDataGenerator(rescale=1.0 / 255.0)\ntest_datagen = preprocessing.image.ImageDataGenerator(rescale=1.0 / 255.0)\n\n# Load the training dataset\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=True,\n    seed=42\n)\n\n# Load the test dataset\ntest_generator = test_datagen.flow_from_directory(\n    test_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=False\n)\n\n# Calculate class weights\nclass_counts = np.bincount(train_generator.classes)\nweight_for_benign_class = class_counts[0] / sum(class_counts)\nweight_for_malignant_class = class_counts[1] / sum(class_counts)\n\nclass_weight = {0: weight_for_benign_class, 1: weight_for_malignant_class}\n\n# Convert generators to TensorFlow datasets for pre-fetching and caching\ntrain_dataset = tf.data.Dataset.from_generator(\n    lambda: train_generator,\n    output_signature=(\n        tf.TensorSpec(shape=(None, img_width, img_height, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None,), dtype=tf.float32)\n    )\n).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()\n\ntest_dataset = tf.data.Dataset.from_generator(\n    lambda: test_generator,\n    output_signature=(\n        tf.TensorSpec(shape=(None, img_width, img_height, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None,), dtype=tf.float32)\n    )\n).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).cache()\n\n# Load the model saved after the first training phase\nmodel = keras.models.load_model('initial_trained_model.keras')\n\n# Re-define the ResNet50 base model \nresnet_base = keras.applications.ResNet101( \n    weights='imagenet',\n    include_top=False,\n    input_shape=(img_width, img_height, 3)\n)\n\n# Unfreeze some layers in the ResNet101 model for fine-tuning\nfor layer in resnet_base.layers[-40:]:  # Unfreezing last 40 layers\n    layer.trainable = True\n\n# Add custom layers to increase model complexity\nmodel = models.Sequential([\n    resnet_base,\n    layers.GlobalAveragePooling2D(),  # Pooling layer for feature extraction\n    layers.Dense(512, activation='relu'),  # Increased number of units for more complexity\n    layers.Dropout(0.4),  # Dropout to avoid overfitting\n    layers.Dense(256, activation='relu'),  # Another dense layer\n    layers.Dropout(0.3),\n    layers.Dense(1, activation='sigmoid')  # Output layer\n])\n\n# Recompile the model after adding custom layers\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate / 10),  \n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Fine-tune the model with the newly unfrozen layers\nhistory_fine_tune = model.fit(\n    train_dataset,\n    steps_per_epoch=train_generator.samples // batch_size,\n    epochs=25,  \n    validation_data=test_dataset,\n    validation_steps=test_generator.samples // batch_size,\n    class_weight=class_weight\n)\n\n# Evaluate the model on the test data\ntest_steps = test_generator.samples // batch_size\ntest_loss, test_accuracy = model.evaluate(test_generator, steps=test_steps)\nprint(f'Test accuracy: {test_accuracy * 100:.2f}%')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T02:25:40.100943Z","iopub.execute_input":"2024-12-05T02:25:40.101344Z","iopub.status.idle":"2024-12-05T02:34:44.100920Z","shell.execute_reply.started":"2024-12-05T02:25:40.101311Z","shell.execute_reply":"2024-12-05T02:34:44.100016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training history\nplt.plot(history_fine_tune.history['accuracy'], label='Train Accuracy')\nplt.plot(history_fine_tune.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\nplt.plot(history_fine_tune.history['loss'], label='Train Loss')\nplt.plot(history_fine_tune.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T02:35:00.532552Z","iopub.execute_input":"2024-12-05T02:35:00.533201Z","iopub.status.idle":"2024-12-05T02:35:01.046580Z","shell.execute_reply.started":"2024-12-05T02:35:00.533166Z","shell.execute_reply":"2024-12-05T02:35:01.045660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nimport seaborn as sns\n\n# Get predictions from the test set\ny_pred = model.predict(test_generator, steps=int(np.ceil(test_generator.samples / batch_size)), verbose=1)\n\n# Convert the probabilities to binary class predictions (0 or 1)\ny_pred_class = (y_pred > 0.5).astype(int)\n\n# Ensure the number of predictions matches the number of true labels\ny_pred_class = y_pred_class[:test_generator.samples]\n\n# Print classification report\nprint(classification_report(test_generator.classes, y_pred_class))\n\n# Confusion Matrix\ncm = confusion_matrix(test_generator.classes, y_pred_class)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices, yticklabels=test_generator.class_indices)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n# ROC-AUC Curve\ny_true = test_generator.classes\ny_pred_prob = y_pred.flatten()  # Get the probability for the positive class (class 1)\n\n# Compute ROC curve\nfpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\nroc_auc = roc_auc_score(y_true, y_pred_prob)\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc='lower right')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T02:35:27.376133Z","iopub.execute_input":"2024-12-05T02:35:27.376793Z","iopub.status.idle":"2024-12-05T02:35:40.268330Z","shell.execute_reply.started":"2024-12-05T02:35:27.376757Z","shell.execute_reply":"2024-12-05T02:35:40.267240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to plot saliency maps\ndef plot_saliency_map(model, image, true_label):\n    image = tf.convert_to_tensor(image)\n    with tf.GradientTape() as tape:\n        tape.watch(image)\n        prediction = model(tf.expand_dims(image, axis=0))\n    \n    # Calculate the gradient of the top predicted class with respect to the input image\n    grads = tape.gradient(prediction, image)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n    \n    # Get the weights for the saliency map\n    saliency_map = tf.reduce_mean(tf.multiply(pooled_grads, image), axis=-1)\n\n    # Plot the image and its saliency map\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.imshow(image.numpy())\n    plt.title(f'True Label: {true_label}')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(saliency_map.numpy(), cmap='hot')\n    plt.title('Saliency Map')\n    plt.axis('off')\n    plt.show()\n\n# Example to plot saliency map for a test image\ntest_image, true_label = next(test_generator)  # Use the built-in `next()` function\nplot_saliency_map(model, test_image[0], true_label[0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get a batch of test data and predictions\ntest_generator.reset()  # Reset the generator to ensure it starts from the beginning\nbatch = next(test_generator)  # Fetch the first batch of test data\nimages, true_labels = batch\npredicted_probs = model.predict(images, verbose=1)\npredicted_labels = (predicted_probs > 0.5).astype(int)  # Convert probabilities to binary predictions\n\n# Map class indices to labels\nclass_indices = {v: k for k, v in test_generator.class_indices.items()}\npredicted_class_names = [class_indices[label[0]] for label in predicted_labels]\ntrue_class_names = [class_indices[label] for label in true_labels]\n\n# Plot the images with predicted and true labels\nplt.figure(figsize=(16, 12))\nfor i in range(len(images)):\n    plt.subplot(4, 4, i + 1)  # Create a grid of 4x4 (adjust if batch size is larger)\n    plt.imshow(images[i])\n    plt.axis('off')\n    plt.title(f\"True: {true_class_names[i]}\\nPred: {predicted_class_names[i]}\")\n    if i == 15:  # Display only the first 16 images\n        break\nplt.tight_layout()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T02:58:36.185135Z","iopub.execute_input":"2024-12-05T02:58:36.185567Z","iopub.status.idle":"2024-12-05T02:58:38.508027Z","shell.execute_reply.started":"2024-12-05T02:58:36.185529Z","shell.execute_reply":"2024-12-05T02:58:38.506857Z"}},"outputs":[],"execution_count":null}]}